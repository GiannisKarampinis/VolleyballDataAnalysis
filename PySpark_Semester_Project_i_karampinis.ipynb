{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMb/cWSovJelqcKCAuAYOt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiannisKarampinis/VolleyballDataAnalysis/blob/main/PySpark_Semester_Project_i_karampinis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data - PySpark - Semester Project\n",
        "You have been asked to process the provided csv file named \"Mens-Volleyball-PlusLiga-2008-2023.csv\" to extract useful data out of it with the utilization of Apache Spark and specifically PySpark."
      ],
      "metadata": {
        "id": "rkjL8XY-Igd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to set up google colab for running PySpark by executing the following code block ([source](https://medium.com/@dipan.saha/pyspark-made-easy-day-2-execute-pyspark-on-google-colabs-f3e57da946a)). After initializing properly the google colab environment, a PySpark Session is necessary to be created here as well.\n"
      ],
      "metadata": {
        "id": "wKHE89uQKT2T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaPuJTs7N87W"
      },
      "outputs": [],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"VolleyballDataAnalysis\").master(\"local[*]\").getOrCreate()\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/sample_data'\n",
        "!git clone 'https://github.com/GiannisKarampinis/VolleyballDataAnalysis'"
      ],
      "metadata": {
        "id": "1O183MTqjEvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://stackoverflow.com/questions/50818788/using-git-clone-in-google-colab-and-using-a-csv-file"
      ],
      "metadata": {
        "id": "n0RW2PoN8JHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_df = spark.read.csv(\"/content/sample_data/VolleyballDataAnalysis/Mens-Volleyball-PlusLiga-2008-2023.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "edSqoZZbOedy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "init_df = init_df.withColumn(\"line_number\", monotonically_increasing_id())\n",
        "init_df.show()"
      ],
      "metadata": {
        "id": "e8C7qAesPV6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make all the necessary imports for the used functions of this project."
      ],
      "metadata": {
        "id": "E9WpgsHgPM9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, upper, concat, col, lit, to_timestamp, concat_ws, when, count, desc\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "from pyspark.sql.types import StringType, StructField, StructType, IntegerType, DoubleType"
      ],
      "metadata": {
        "id": "5gz7I1qpQZ6D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 1\n",
        "For columns **percentage_cols = ['T1_Srv_Eff', 'T1_Rec_Pos', 'T1_Rec_Perf', 'T1_Att_Kill_Perc', 'T1_Att_Eff', 'T1_Att_Sum', 'T2_Srv_Eff', 'T2_Rec_Pos', 'T2_Rec_Perf', 'T2_Att_Kill_Perc', 'T2_Att_Eff', 'T2_Att_Sum']**, remove percentage sign ('%') and depict the result without truncation.\n"
      ],
      "metadata": {
        "id": "_qRJPgevNr4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 1:\n",
        "df1 = init_df  # modify the copy (df1) and not the original df\n",
        "\n",
        "percentage_cols = ['T1_Srv_Eff', 'T1_Rec_Pos', 'T1_Rec_Perf', 'T1_Att_Kill_Perc', 'T1_Att_Eff',\n",
        "                    'T1_Att_Sum', 'T2_Srv_Eff', 'T2_Rec_Pos', 'T2_Rec_Perf', 'T2_Att_Kill_Perc',\n",
        "                    'T2_Att_Eff', 'T2_Att_Sum']\n",
        "\n",
        "for colu in percentage_cols:\n",
        "    df1 = df1.withColumn(\"{}\".format(colu), regexp_replace(\"{}\".format(colu), '%', ''))\n",
        "df1.show(truncate=False)"
      ],
      "metadata": {
        "id": "YiiDaqv4QJcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 2\n",
        "Convert team names to uppercase."
      ],
      "metadata": {
        "id": "FDUOHO20OZOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 2:\n",
        "for colu in ['Team_1', 'Team_2']:\n",
        "    df1 = df1.withColumn(colu, upper(colu))\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "FAjLIzv6Qp-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 3\n",
        "Calculate and save to a variable the number of games read from csv."
      ],
      "metadata": {
        "id": "ARtQFYRtOk6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 3:\n",
        "numOfGames = df1.count()\n",
        "print(numOfGames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI-EEtb9Qy29",
        "outputId": "2430706d-9e53-41fc-af98-af95bdd99f1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 4\n",
        "Calculate the number of sets per match."
      ],
      "metadata": {
        "id": "gqZWBWhTO1GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 4:\n",
        "df2 = init_df\n",
        "df2 = df2.withColumn(\"Sets_per_Game\", sum(df2[colu] for colu in [\"T1_Score\", \"T2_Score\"]))\n",
        "#df3 = df2.select(\"Sets_per_Game\")\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "bdzPUNHlQ3VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 5\n",
        "Calculate the total number of games per total number of sets."
      ],
      "metadata": {
        "id": "Tp4MbPu8QlGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 5:\n",
        "df3 = df2.select(\"Sets_per_Game\")\n",
        "\n",
        "# Total sets:\n",
        "l2 = df3.groupBy().sum().collect()[0][0]\n",
        "\n",
        "data = [(numOfGames/l2,)]\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.coalesce(1).write.csv('/content/sample_data/games_div_by_sets', header = False, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "x1mRYD-eQ59S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st Exercise - 6\n",
        "Calculate and write to a csv file the number of games that each team took place (notice that the team could be either home or away)."
      ],
      "metadata": {
        "id": "lJAzVcBVTerB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st Exercise - 6:\n",
        "df6 = df1\n",
        "\n",
        "home_teams = df6.select('Team_1')\n",
        "away_teams = df6.select('Team_2')\n",
        "\n",
        "un_df = home_teams.select('Team_1').union(away_teams.select('Team_2')).withColumnRenamed(\"Team_1\", \"Team\")\n",
        "un_df = un_df.groupBy(\"Team\").count()\n",
        "\n",
        "# PySpark will write a lot of csv files in parallel from its nature.\n",
        "# We will use coalesce to merge those data in a single csv file.\n",
        "# source: https://sparkbyexamples.com/spark/spark-write-dataframe-single-csv-file/\n",
        "# However, caution should be taken in case of very very large data sets as coalesce is a computationally expensive operation.\n",
        "\n",
        "un_df.coalesce(1).write.csv('/content/sample_data/num_of_matches_per_team', header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "kagsS4K8Q9Yj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd Exercise\n",
        "Create and save to a csv file a holistic data analysis table for all the teams found inside the initial csv file. **For each team**, present in descending order per total games won:\n",
        "1.   Number of games (home, away and total).\n",
        "2.   Sets won and lost.\n",
        "3.   Points won and lost.\n"
      ],
      "metadata": {
        "id": "Ee1AG1LKXiXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd Exercise - 1:\n",
        "# OUTPUT THIS TO CSV\n",
        "\n",
        "# df6 is the initial dataframe for each subsequent task of exercise 2.\n",
        "\n",
        "# To find out how many times a team won, we could find the number of times team won as a HomeTeam when Winner==0 and AwayTeam when Winner==1.\n",
        "\n",
        "# How many times team won as a Home Team:\n",
        "df7 = df6.select(['Team_1', 'Winner'])\n",
        "home_games = df7.groupBy('Team_1').count().withColumnRenamed(\"count\", \"HomeGames\")\n",
        "home_games = home_games.withColumnRenamed(\"Team_1\", \"Team\")\n",
        "home_games_won = df7.groupBy('Team_1').agg(count(when(col(\"Winner\") == 0, True)))\n",
        "home_games_won = home_games_won.withColumnRenamed(\"Team_1\", \"Team\")\n",
        "\n",
        "#home_games.show()\n",
        "#home_games_won.show()\n",
        "\n",
        "# How many times team won as an Away Team:\n",
        "df8 = df6.select([\"Team_2\", \"Winner\"])\n",
        "away_games = df8.groupBy('Team_2').count().withColumnRenamed(\"count\", \"AwayGames\")\n",
        "away_games = away_games.withColumnRenamed(\"Team_2\", \"Team\")\n",
        "away_games_won = df8.groupBy('Team_2').agg(count(when(col(\"Winner\") == 1, True)))\n",
        "away_games_won = away_games_won.withColumnRenamed(\"Team_2\", \"Team\")\n",
        "\n",
        "#away_games.show()\n",
        "#away_games_won.show()\n",
        "\n",
        "# Join the results from the previous 2 dataframes:\n",
        "joined_df1 = un_df.alias(\"un_df\")\\\n",
        "    .join(home_games_won.alias(\"home_games_won\"), col(\"un_df.Team\") == col(\"home_games_won.Team\"))\\\n",
        "    .join(away_games_won.alias(\"away_games_won\"), col(\"un_df.Team\") == col(\"away_games_won.Team\"))\\\n",
        "    .join(home_games.alias(\"home_games\"), col(\"un_df.Team\") == col(\"home_games.Team\")) \\\n",
        "    .join(away_games.alias(\"away_games\"), col(\"un_df.Team\") == col(\"away_games.Team\")) \\\n",
        "    .select(\"un_df.Team\", \"un_df.count\", \"count(CASE WHEN (Winner = 0) THEN true END)\", \"count(CASE WHEN (Winner = 1) THEN true END)\", \"home_games.HomeGames\", \"away_games.AwayGames\")\n",
        "\n",
        "joined_df1 = joined_df1.withColumnRenamed(\"count(CASE WHEN (Winner = 0) THEN true END)\", \"HomeGamesWon\")\n",
        "joined_df1 = joined_df1.withColumnRenamed(\"count(CASE WHEN (Winner = 1) THEN true END)\", \"AwayGamesWon\")\n",
        "joined_df1 = joined_df1.withColumn(\"TotalGamesWon\", col(\"HomeGamesWon\") + col(\"AwayGamesWon\"))\n",
        "joined_df1 = joined_df1.withColumnRenamed(\"count\", \"TotalGames\")\n",
        "joined_df1.show()"
      ],
      "metadata": {
        "id": "aWWbYbrkRVMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd Exercise - 2:\n",
        "\n",
        "sets = df6.select(\"Team_1\", \"Team_2\", \"T1_Score\", \"T2_Score\", \"T1_Sum\", \"T2_Sum\", \"Winner\")\n",
        "\n",
        "sets_home_won = sets.groupBy(\"Team_1\").agg(_sum(sets.T1_Score))\n",
        "sets_away_won = sets.groupBy(\"Team_2\").agg(_sum(sets.T2_Score))\n",
        "\n",
        "sets_won = sets_home_won.alias(\"sets_home_won\").join(sets_away_won.alias(\"sets_away_won\"), col(\"sets_home_won.Team_1\") == col(\"sets_away_won.Team_2\"))\n",
        "\n",
        "sets_won = sets_won.withColumnRenamed(\"sum(T1_Score)\", \"SetsWonAsHomeTeam\")\n",
        "sets_won = sets_won.withColumnRenamed(\"sum(T2_Score)\", \"SetsWonAsAwayTeam\")\n",
        "\n",
        "#sets_won.show()\n",
        "\n",
        "f_sets_won = sets_won.withColumn(\"SetsWon\", col(\"SetsWonAsHomeTeam\") + col(\"SetsWonAsAwayTeam\"))\n",
        "\n",
        "f_sets_won = f_sets_won.withColumnRenamed(\"Team_1\", \"Team_1_won\")\n",
        "f_sets_won = f_sets_won.drop(\"SetsWonAsHomeTeam\")\n",
        "f_sets_won = f_sets_won.drop(\"SetsWonAsAwayTeam\")\n",
        "f_sets_won = f_sets_won.drop(\"Team_2\")\n",
        "\n",
        "#f_sets_won.show()\n",
        "\n",
        "sets_home_lost = sets.groupBy(\"Team_1\").agg(_sum(col(\"T2_Score\")))\n",
        "sets_away_lost = sets.groupBy(\"Team_2\").agg(_sum(col(\"T1_Score\")))\n",
        "\n",
        "sets_lost = sets_home_lost.alias(\"sets_home_lost\").join(sets_away_lost.alias(\"sets_away_lost\"),\n",
        "                                                      col(\"sets_home_lost.Team_1\") == col(\"sets_away_lost.Team_2\"))\n",
        "sets_lost = sets_lost.withColumnRenamed(\"sum(T2_Score)\", \"SetsLostAsHomeTeam\")\n",
        "sets_lost = sets_lost.withColumnRenamed(\"sum(T1_Score)\", \"SetsLostAsAwayTeam\")\n",
        "\n",
        "#sets_lost.show()\n",
        "\n",
        "f_sets_lost = sets_lost.withColumn(\"SetsLost\", col(\"SetsLostAsHomeTeam\") + col(\"SetsLostAsAwayTeam\"))\n",
        "\n",
        "f_sets_lost = f_sets_lost.withColumnRenamed(\"Team_1\", \"Team_1_lost\")\n",
        "f_sets_lost= f_sets_lost.drop(\"SetsLostAsHomeTeam\")\n",
        "f_sets_lost = f_sets_lost.drop(\"SetsLostAsAwayTeam\")\n",
        "f_sets_lost = f_sets_lost.drop(\"Team_2\")\n",
        "\n",
        "#f_sets_lost.show()\n",
        "\n",
        "joined_df1 = joined_df1.alias(\"joined_df1\")\\\n",
        "            .join(f_sets_won.alias(\"f_sets_won\"), col(\"joined_df1.Team\") == col(\"f_sets_won.Team_1_won\"))\\\n",
        "            .join(f_sets_lost.alias(\"f_sets_lost\"), col(\"joined_df1.Team\") == col(\"f_sets_lost.Team_1_lost\"))\n",
        "\n",
        "joined_df1 = joined_df1.drop(\"Team_1_lost\")\n",
        "joined_df1 = joined_df1.drop(\"Team_1_won\")\n",
        "joined_df1.show()"
      ],
      "metadata": {
        "id": "a3EZkM7xRuqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rF9pO60X9vZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd Exercise - 3:\n",
        "points_home_won = sets.groupBy(\"Team_1\").agg(_sum(sets.T1_Sum))\n",
        "points_away_won = sets.groupBy(\"Team_2\").agg(_sum(sets.T2_Sum))\n",
        "\n",
        "#points_home_won.show()\n",
        "\n",
        "points_won = points_home_won.alias(\"points_home_won\").join(points_away_won.alias(\"points_away_won\"),\n",
        "                                                      col(\"points_home_won.Team_1\") == col(\"points_away_won.Team_2\"))\n",
        "\n",
        "points_won = points_won.withColumnRenamed(\"sum(T1_Sum)\", \"pointsWonAsHomeTeam\")\n",
        "points_won = points_won.withColumnRenamed(\"sum(T2_Sum)\", \"pointsWonAsAwayTeam\")\n",
        "\n",
        "#points_won.show()\n",
        "\n",
        "f_points_won = points_won.withColumn(\"pointsWon\", col(\"pointsWonAsHomeTeam\") + col(\"pointsWonAsAwayTeam\"))\n",
        "\n",
        "f_points_won = f_points_won.withColumnRenamed(\"Team_1\", \"Team_1_won\")\n",
        "f_points_won = f_points_won.drop(\"pointsWonAsHomeTeam\")\n",
        "f_points_won = f_points_won.drop(\"pointsWonAsAwayTeam\")\n",
        "f_points_won = f_points_won.drop(\"Team_2\")\n",
        "\n",
        "#f_points_won.show()\n",
        "\n",
        "\n",
        "points_home_lost = sets.groupBy(\"Team_1\").agg(_sum(sets.T2_Sum))\n",
        "points_away_lost = sets.groupBy(\"Team_2\").agg(_sum(sets.T1_Sum))\n",
        "\n",
        "points_lost = points_home_lost.alias(\"points_home_lost\").join(points_away_lost.alias(\"points_away_lost\"),\n",
        "                                                      col(\"points_home_lost.Team_1\") == col(\"points_away_lost.Team_2\"))\n",
        "points_lost = points_lost.withColumnRenamed(\"sum(T2_Sum)\", \"pointsLostAsHomeTeam\")\n",
        "points_lost = points_lost.withColumnRenamed(\"sum(T1_Sum)\", \"pointsLostAsAwayTeam\")\n",
        "\n",
        "#points_lost.show()\n",
        "\n",
        "f_points_lost = points_lost.withColumn(\"pointsLost\", col(\"pointsLostAsHomeTeam\") + col(\"pointsLostAsAwayTeam\"))\n",
        "\n",
        "f_points_lost = f_points_lost.withColumnRenamed(\"Team_1\", \"Team_1_lost\")\n",
        "f_points_lost= f_points_lost.drop(\"pointsLostAsHomeTeam\")\n",
        "f_points_lost = f_points_lost.drop(\"pointsLostAsAwayTeam\")\n",
        "f_points_lost = f_points_lost.drop(\"Team_2\")\n",
        "\n",
        "#f_points_lost.show()\n",
        "\n",
        "joined_df1 = joined_df1.alias(\"joined_df1\")\\\n",
        "            .join(f_points_won.alias(\"f_points_won\"), col(\"joined_df1.Team\") == col(\"f_points_won.Team_1_won\"))\\\n",
        "            .join(f_points_lost.alias(\"f_points_lost\"), col(\"joined_df1.Team\") == col(\"f_points_lost.Team_1_lost\"))\n",
        "\n",
        "\n",
        "joined_df1 = joined_df1.drop(\"Team_1_lost\")\n",
        "joined_df1 = joined_df1.drop(\"Team_1_won\")\n",
        "\n",
        "joined_df1 = joined_df1.orderBy(desc(\"TotalGamesWon\"))\n",
        "joined_df1 = joined_df1.select(\"Team\", \"HomeGames\", \"AwayGames\", \"TotalGames\", \"SetsWon\", \"SetsLost\", \"PointsWon\", \"PointsLost\")\n",
        "joined_df1.coalesce(1).write.csv('/content/sample_data/final_output', header = True, mode=\"overwrite\")\n",
        "joined_df1.show(truncate=False)"
      ],
      "metadata": {
        "id": "LovvAiJaSrCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}